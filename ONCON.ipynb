{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 align=\"center\">TEXT CLASSIFICATION USING SVM</h3> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"svm2.PNG\" width=\"400\" height=\"250\">![](/svm2.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is text classification?\n",
    "Given a truck-load of textual data, it is a huge task to analyse what is inside it owing to its lack of structure. \n",
    "If machines can help us to automate this mechanical process of grouping the text, it is indeed substantial! \n",
    "\n",
    "Text classification is a way of identifying the category in which the contents of a text belong.\n",
    "Depending on the scenario, this classification can be binary (positive/negative, spam/non-spam) or \n",
    "categorical (politics, technology business, fashion, sports etc). \n",
    "\n",
    "Classification of textual data is a means to clean it, organize it, make it user-friendly and to make sense out of the unstructured data.\n",
    "It marks its applications in the field of spam detection, sentiment analysis, tagging, language detection and a lot more."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How is it done?\n",
    "There are numerous machine learning algorithms like Naive Bayes, Random Forest, Support Vector Machines, \n",
    "Neural Models which make use of the training data to arrive at the conclusive category for the new, \n",
    "previously unseen text data.\n",
    "\n",
    "Steps:\n",
    "1. Data download.\n",
    "2. Data pre-processing: xml-parsing, lower-case conversion, punctuation and special character removals, stopwords removal, lemmatization etc, all depending upon the requirement!\n",
    "3. Word-to-vector Conversion: Machine do not understand text, so we vectorize every word into numerics and feed the vectors thus formed in the machine learning model.\n",
    "4. Implementing the algorithm.\n",
    "\n",
    "\n",
    "Here in this article, we will discuss one of these algorithms, Support Vector Machines from scratch.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"svm.PNG\" width=\"800\" height=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is Support Vector Machine (SVM) ?\n",
    "Support Vector Machines or SVM as we call them, are based on the concept of ‘lines’ or ‘hyperplanes’ dividing the space of\n",
    "vectorized data into two or multiple subspaces. \n",
    "\n",
    "In layman terms, SVM first analyses the given points in the n-dimensional space, then figures out the line/hyperplane that separates the points. Then, points belonging to one category falls onto one side of the line/hyperplane and points of another category falls exactly into its opposite side. \n",
    "SVM model, once trained over the training data, puts the test point in the vector space and analyses its respective position with the line/hyperplane and hence decides its category!\n",
    "\n",
    "A linear classifier has the form:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"linear.PNG\" width=\"400\" height=\"250\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In 3D it is plane, while in nD we call it a hyperplane.\n",
    "But data points are rarely linearly separable or they are so intricately mixed that they are not even separable. In that complex case, say data points are not separable in x-y plane, then we add another dimension z, and\n",
    "plot the 2D space points into 3D use a hyperplane to separate and transform that hyperplane \n",
    "back to the original 2D space, thereby getting a separation in the original space. We call these transformations as ‘Kernels’.\n",
    "\n",
    "As a matter of fact, there can be multiple hyperplanes separating the data-points. Which one should be chosen as the decision boundary?\n",
    "The one that maximizes the smallest distance between the data points of both the classes seems to be a natural choice,\n",
    "providing better margins. Support vectors are the sample data points which lie clode to the decision boundary.\n",
    "\n",
    "The loss function, which is a function of data point, prediction class and actual label, is actually a measure of the penaly for wrong prediction.\n",
    "SVM uses Hinge loss as the loss function, given as below:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"hinge.PNG\" width=\"400\" height=\"250\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It tells us how better we are doing with respect to one training example. When we sum over all the training examples, what we get is the cost function. \n",
    "\n",
    "The optimization problem finally looks like:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"cost.PNG\" width=\"400\" height=\"250\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where R(w) is the regularization function used to tackle overfiiting, \n",
    "C is the scalar constant,\n",
    "and L is the loss fuction.\n",
    "\n",
    "Below is the python implementation of SVM using scikit learn library"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv \n",
    "import re\n",
    "import xml.etree.ElementTree as ET \n",
    "import os\n",
    "import pandas as pd\n",
    "import string\n",
    "import nltk\n",
    "from string import punctuation\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = r'C:\\Users\\Aditi Sethia\\HTML\\training\\pan12.xml'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parsing the xml file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree = ET.parse(data)\n",
    "myroot = tree.getroot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getvalueofnode(node):\n",
    "    return node.text if node is not None else None\n",
    "def rem_num(text):\n",
    "    output = re.sub('[0-9]+', '', text)\n",
    "    return output\n",
    "def use(x):\n",
    "    o = 0\n",
    "    for i in x:\n",
    "        if i in lis:\n",
    "            o = o + 1\n",
    "            return 1\n",
    "    if o == 0:\n",
    "        return 0    \n",
    "\n",
    "def rem_stopword(text):\n",
    "    tokens = text.split()\n",
    "    result = [i for i in tokens if not i in stop_words]\n",
    "    return (' '.join(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = ''\n",
    "d = 0\n",
    "df = pd.DataFrame(columns = ['Users','Conversation_ID','text'])\n",
    "for i in myroot:\n",
    "    user = {}\n",
    "    k = ''\n",
    "    p = i.attrib.get('id')\n",
    "    for j in i:\n",
    "        l = j.find('text')\n",
    "        k = k + ' ' + str(getvalueofnode(l))\n",
    "        auth = j.find('author')\n",
    "        auth = getvalueofnode(auth)\n",
    "        if auth in user:\n",
    "            user[auth] = user[auth] + 1\n",
    "        else:\n",
    "            user[auth] = 1\n",
    "    list_user = [i for i in user]\n",
    "    df = df.append({'Users' :list_user  ,'Conversation_ID': p,'text':k},ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text-preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_preprocessing(df):\n",
    "    df[\"text\"] = df['text'].apply(lambda x: x.lower())\n",
    "    df[\"text\"] = df['text'].apply(lambda x: ''.join(c for c in x if c not in punctuation))\n",
    "    df['text'] = df['text'].apply(rem_num)\n",
    "text_preprocessing(df = df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "file1 = open(r\"\\training\\tcorpus.txt\",\"r+\")\n",
    "lis = file1.readlines()\n",
    "lis = [i[:-1] for i in lis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def work(df):\n",
    "    df['Predator_Present'] = df['Users'].apply(use) \n",
    "work(df = df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import resample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_majority = df[df['Predator_Present'] == 0]\n",
    "df_minority = df[df['Predator_Present'] == 1]\n",
    " \n",
    "# Downsample majority class\n",
    "df_majority_downsampled = resample(df_majority, \n",
    "                                 replace=False,    # sample without replacement\n",
    "                                 n_samples=4000,     # to match minority class\n",
    "                                 random_state=123) # reproducible results\n",
    " \n",
    "# Combine minority class with downsampled majority class\n",
    "df_downsampled = pd.concat([df_majority_downsampled, df_minority])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Users</th>\n",
       "      <th>Conversation_ID</th>\n",
       "      <th>text</th>\n",
       "      <th>Predator_Present</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>66688</th>\n",
       "      <td>[fac3a2081264f1dbb943eaf7165d8fc3]</td>\n",
       "      <td>cfac30bed30bc0ed991787bdc42486b4</td>\n",
       "      <td>sorry about that big lighting strike knocked ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66821</th>\n",
       "      <td>[c62283536cf6261e5ffbcb323c8a2571, adb9f962493...</td>\n",
       "      <td>1c7e6180909437ab3ab8191443e09e2e</td>\n",
       "      <td>just wanted to let you know im canceling my i...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66880</th>\n",
       "      <td>[3e97c68b68f9aa0fb7d705a65c6a8443]</td>\n",
       "      <td>52a4e73525be80549ed86d6bb1458804</td>\n",
       "      <td>hello how was your sking</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66906</th>\n",
       "      <td>[e4c7c376bbd07aeb4a59684a2b94a664]</td>\n",
       "      <td>a0b239c1a240bfe148928ddda8485f83</td>\n",
       "      <td>sorry i had to go so soon ttyl</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66926</th>\n",
       "      <td>[74bfc043bd5ce9c17b37ffae6e0ba2fa, 8cd850ea421...</td>\n",
       "      <td>4ed6b02ae537fdfd6078597b706292a8</td>\n",
       "      <td>hay remember me oh yeah hi how could i forget...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   Users  \\\n",
       "66688                 [fac3a2081264f1dbb943eaf7165d8fc3]   \n",
       "66821  [c62283536cf6261e5ffbcb323c8a2571, adb9f962493...   \n",
       "66880                 [3e97c68b68f9aa0fb7d705a65c6a8443]   \n",
       "66906                 [e4c7c376bbd07aeb4a59684a2b94a664]   \n",
       "66926  [74bfc043bd5ce9c17b37ffae6e0ba2fa, 8cd850ea421...   \n",
       "\n",
       "                        Conversation_ID  \\\n",
       "66688  cfac30bed30bc0ed991787bdc42486b4   \n",
       "66821  1c7e6180909437ab3ab8191443e09e2e   \n",
       "66880  52a4e73525be80549ed86d6bb1458804   \n",
       "66906  a0b239c1a240bfe148928ddda8485f83   \n",
       "66926  4ed6b02ae537fdfd6078597b706292a8   \n",
       "\n",
       "                                                    text  Predator_Present  \n",
       "66688   sorry about that big lighting strike knocked ...                 1  \n",
       "66821   just wanted to let you know im canceling my i...                 1  \n",
       "66880                           hello how was your sking                 1  \n",
       "66906                    sorry i had to go so soon ttyl                  1  \n",
       "66926   hay remember me oh yeah hi how could i forget...                 1  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_downsampled.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6016, 4)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_downsampled.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Downsampling of data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import resample\n",
    "\n",
    "df_majority = df[df['Predator_Present'] == 0]\n",
    "df_minority = df[df['Predator_Present'] == 1]\n",
    " \n",
    "# Downsample majority class\n",
    "df_majority_downsampled = resample(df_majority, \n",
    "                                 replace=False, \n",
    "                                 n_samples=4032,    \n",
    "                                 random_state=123) \n",
    " \n",
    "\n",
    "df_downsampled = pd.concat([df_majority_downsampled, df_minority])\n",
    "\n",
    "y = df_downsampled['Predator_Present']\n",
    "y = y.values\n",
    "\n",
    "corpus_DS = []\n",
    "for i in df_downsampled[\"text\"]:\n",
    "    corpus_DS += [i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Converting the data-set into vectors (Tf-idf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6048, 33425)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer(stop_words = 'english')\n",
    "X_vect = vectorizer.fit_transform(corpus_DS).toarray()\n",
    "X_vect.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Splitting the data into training and testing samples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_vect, y, test_size = 0.2, random_state = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fitting the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = SVC(kernel = 'linear', random_state = 0)\n",
    "classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = classifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.98      0.94       800\n",
      "           1       0.96      0.79      0.87       410\n",
      "\n",
      "    accuracy                           0.92      1210\n",
      "   macro avg       0.93      0.89      0.90      1210\n",
      "weighted avg       0.92      0.92      0.92      1210\n",
      "\n",
      "0.9029627633638719\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(metrics.f1_score(y_test, y_pred, average='macro'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "References:\n",
    "1. https://monkeylearn.com/text-classification/\n",
    "2. https://medium.com/@bedigunjit/simple-guide-to-text-classification-nlp-using-svm-and-naive-bayes-with-python-421db3a72d34"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

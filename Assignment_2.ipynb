{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Thank you so much.  That's so nice.  Isn't he a great guy.  He doesn't get a fair press  he doesn't get it.  It's just not fair.  And I have to tell you I'm here  and very strongly here  because I have \""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "from urllib import request\n",
    "from sklearn.model_selection import train_test_split\n",
    "url = \"https://raw.githubusercontent.com/ryanmcdermott/trump-speeches/master/speeches.txt\"\n",
    "response = request.urlopen(url)\n",
    "text = response.read().decode('utf8')\n",
    "type(text)\n",
    "text=re.sub('\\r','',text)\n",
    "text=re.sub('\\n','',text)\n",
    "text=re.sub('\\ufeffSPEECH 1...','',text)\n",
    "text=re.sub(\"[^A-Za-z.']\",' ',text)\n",
    "text[:202]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11626"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#tokenizing sentences\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tokenize import word_tokenize\n",
    "sentences = nltk.sent_tokenize(text)\n",
    "\n",
    "len(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#splitting into training and test data\n",
    "\n",
    "train_data,test_data=train_test_split(sentences,test_size=0.2,random_state=40)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "text1=[]\n",
    "for each in sentences:  \n",
    "    text1.append(\"<s> \" + each + \" </s>\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "words=[]\n",
    "for i in sentences:\n",
    "        words_token = word_tokenize(i)\n",
    "        for j in words_token:\n",
    "            words.append(j.lower())                \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unigram MLE for 'and':  0.026714114695222946\n"
     ]
    }
   ],
   "source": [
    "#counting MLE for unigrams\n",
    "\n",
    "def unigrams(word):\n",
    "    word=word.lower()\n",
    "    return float(words.count(word)/len(words))\n",
    "print(\"unigram MLE for 'and': \", unigrams('and'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bigram MLE for 'they are':  0.0006286317147433269\n"
     ]
    }
   ],
   "source": [
    "#counting MLE for bigrams\n",
    "\n",
    "bigram=zip(words,words[1:])\n",
    "bigram=list(bigram)\n",
    "bigram[:10]\n",
    "\n",
    "def bigrams(word1,word2):\n",
    "    word1=word1.lower();\n",
    "    word2=word2.lower();\n",
    "    return float(bigram.count((word1,word2))/words.count(word1))*unigrams(word1)\n",
    "\n",
    "print(\"bigram MLE for 'they are': \", bigrams('they','are'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trigram MLE for 'for a long':  0.0001257263429486654\n"
     ]
    }
   ],
   "source": [
    "#counting MLE for trigrams\n",
    "\n",
    "trigram=zip(words,words[1:],words[2:])\n",
    "trigram=list(trigram)\n",
    "trigram[:10]\n",
    "\n",
    "def trigrams(word1,word2,word3):\n",
    "    word1=word1.lower()\n",
    "    word2=word2.lower()\n",
    "    word3=word3.lower()\n",
    "    P=(float(trigram.count((word1,word2,word3)))/bigram.count((word1,word2)))*(bigrams(word1,word2))\n",
    "    return P;\n",
    "\n",
    "print(\"trigram MLE for 'for a long': \", trigrams('for','a','long'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quadgram MLE for 'for a long time':  9.292816652727442e-05\n"
     ]
    }
   ],
   "source": [
    "#counting MLE for quadgrams\n",
    "\n",
    "quadgram= zip(words, words[1:],words[2:],words[3:])\n",
    "quadgram=list(quadgram)\n",
    "quadgram[:10]\n",
    "\n",
    "def quadgrams(word1,word2,word3,word4):\n",
    "    word1,word2,word3,word4=word1.lower(),word2.lower(),word3.lower(),word4.lower();\n",
    "    P=(float(quadgram.count((word1,word2,word3,word4)))/trigram.count((word1,word2,word3)))*(trigrams(word1,word2,word3))  \n",
    "    return P\n",
    "\n",
    "\n",
    "print(\"Quadgram MLE for 'for a long time': \", quadgrams('for', 'a', 'long', 'time'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of possible unigrams =  8289\n",
      "actual number of unigrams = 8289\n",
      "number of possible bigrams =  68707521\n",
      "actual number of bigrams = 53370\n",
      "number of possible trigrams =  569516641569\n",
      "actual number of trigrams = 111969\n",
      "number of possible quadgrams =  4720723441965441\n",
      "actual number of quadgrams = 149001\n"
     ]
    }
   ],
   "source": [
    "print(\"number of possible unigrams = \",len(set((words))))\n",
    "print(\"actual number of unigrams =\",len(set(words)))\n",
    "\n",
    "print(\"number of possible bigrams = \",len(set(words))*len((set(words))))\n",
    "print(\"actual number of bigrams =\",len(set(bigram)))\n",
    "\n",
    "print(\"number of possible trigrams = \",len(set(words))*len(set(words))*len(set(words)))\n",
    "print(\"actual number of trigrams =\",len(set(trigram)))\n",
    "\n",
    "print(\"number of possible quadgrams = \",len(set(words))*len(set(words))*len(set(words))*len(set(words)))\n",
    "print(\"actual number of quadgrams =\", len(set(quadgram)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sentence generation using n-grams\n",
    "\n",
    "import random\n",
    "from collections import defaultdict\n",
    "\n",
    "def Generate(model):\n",
    "    if model==bigram:\n",
    "        word_dict=defaultdict(list)            #word_dict contains all the words that follow the given key-word in the corpus.\n",
    "        for word, next_word in bigram:\n",
    "            word_dict[word].append(next_word)\n",
    "        current = \".\" \n",
    "        result = []\n",
    "        while True:\n",
    "            next_word_candidates = word_dict[word] \n",
    "            current = random.choice(next_word_candidates) \n",
    "            result.append(current)\n",
    "            if current == \".\":\n",
    "                return \" \".join(result) \n",
    "            \n",
    "            \n",
    "    if model==trigram:\n",
    "        trigram_transitions = defaultdict(list)\n",
    "        starts = []\n",
    "        for prev, current, next in trigram:\n",
    "            if prev == \".\": \n",
    "                starts.append(current)\n",
    "            trigram_transitions[(prev, current)].append(next)\n",
    "        \n",
    "        current = random.choice(starts) \n",
    "        prev = \".\"\n",
    "        result = [current]\n",
    "        while True:\n",
    "            next_word_candidates = trigram_transitions[(prev, current)]\n",
    "            next_word = random.choice(next_word_candidates)\n",
    "            prev, current = current, next_word\n",
    "            result.append(current)\n",
    "            if current == \".\":\n",
    "                return \" \".join(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentences generated using bigrams:\n",
      "have re know .\n",
      "a noticed weren what know keep noticed ve you said ve know know look ve all would look know know saw that ve know re can think ve said know proud re believe remember go wouldn need re about on going go .\n",
      "know get know don ve have know very know know know folks can so thank just talk .\n",
      "that understand take very .\n",
      "like will find .\n"
     ]
    }
   ],
   "source": [
    "print(\"Sentences generated using bigrams:\")\n",
    "for i in range(5):\n",
    "    print(Generate(bigram))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentences generated using trigrams:\n",
      "i thought it was just brought up by more than .\n",
      "it is ... i always tell people i think are horrible people .\n",
      "you know why i m in for a number one that made me so i mentioned corporate inversion .\n",
      "i ll tell you that there is a disaster .\n",
      "we re going to come off .\n"
     ]
    }
   ],
   "source": [
    "print(\"Sentences generated using trigrams:\")\n",
    "for i in range(5):\n",
    "    print(Generate(trigram))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log P=-7.0799445078429795 P=0.000841819861482368\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-7.0799445078429795"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#bigram probability\n",
    "\n",
    "import math\n",
    "def Prob_bigram(sentence):\n",
    "    log_P=0\n",
    "    Words=word_tokenize(sentence)\n",
    "    for i in range(len(Words)):\n",
    "        Words[i]=Words[i].lower()\n",
    "    for i in range(len(Words)-1):\n",
    "        log_P+=math.log(bigrams(Words[i],Words[i+1]))\n",
    "        \n",
    "\n",
    "    print(\"log P=\" +str(log_P)+\" P=\"+str(math.exp(log_P)))\n",
    "    return log_P\n",
    "\n",
    "Prob_bigram('thank you')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log P=-16.49182632069253 P=6.881622297882457e-08\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-16.49182632069253"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def Prob_trigram(sentence):\n",
    "    log_P=0\n",
    "    Words=word_tokenize(sentence)\n",
    "    for i in range(len(Words)):\n",
    "        Words[i]=Words[i].lower()\n",
    "    for i in range(len(Words)-2):\n",
    "        log_P+=math.log(trigrams(Words[i],Words[i+1],Words[i+2]))\n",
    "        \n",
    "\n",
    "    print(\"log P=\" +str(log_P)+\" P=\"+str(math.exp(log_P)))\n",
    "    return log_P\n",
    "\n",
    "Prob_trigram('thank you very much')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "34.465962882278475"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def perplexity_bigram(sentence):\n",
    "    P=1;\n",
    "    Words=word_tokenize(sentence)\n",
    "    for i in range(len(Words)):\n",
    "        Words[i]=Words[i].lower()\n",
    "    for i in range(len(Words)-1):\n",
    "        P=P*bigrams(Words[i],Words[i+1])\n",
    "        Perp=(1/float(P))**(1/float(len(Words)))\n",
    "    return Perp\n",
    "\n",
    "perplexity_bigram('thank you')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "61.74151642088205"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def perplexity_trigram(sentence):\n",
    "    P=1\n",
    "    Words=word_tokenize(sentence)\n",
    "    for i in range(len(Words)):\n",
    "        Words[i]=Words[i].lower()\n",
    "    for i in range(len(Words)-2):\n",
    "        P=P*trigrams(Words[i],Words[i+1],Words[i+2])\n",
    "        Perp=(1/float(P))**(1/float(len(Words)))\n",
    "    return Perp\n",
    "\n",
    "perplexity_trigram('thank you very much')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#RNN/LSTM model\n",
    "dict1 = {}\n",
    "dict2={}\n",
    "\n",
    "for j in range(len(sentences)):\n",
    "    dict1[sentences[j]] = j\n",
    "\n",
    "dict2 = dict(zip(dict1.values(), dict2.keys()))\n",
    "test_len = len(test_data)\n",
    "\n",
    "def word_to_int(sent,dict2):\n",
    "    result = []\n",
    "    for word in sent:\n",
    "        result.append(dict2[word])\n",
    "    return result\n",
    "\n",
    "\n",
    "# Parameters\n",
    "learning_rate = 0.00001\n",
    "training_iters = len(train_data)*2\n",
    "display_step = 10000\n",
    "n_input = 5\n",
    "\n",
    "# number of units in RNN/LSTM cell\n",
    "n_hidden = 128\n",
    "x = tf.placeholder(\"float\", [None, n_input, 1])\n",
    "y = tf.placeholder(\"float\", [None, vocab_l])\n",
    "weights = { 'out': tf.Variable(tf.random_normal([n_hidden, vocab_l]))}\n",
    "biases = {'out': tf.Variable(tf.random_normal([vocab_l]))}\n",
    "\n",
    "\n",
    "\n",
    "x = tf.placeholder(\"float\", [None, n_input, 1])\n",
    "y = tf.placeholder(\"float\", [None, vocab_l])\n",
    "\n",
    "\n",
    "weights = {'out': tf.Variable(tf.random_normal([n_hidden, vocab_l]))}\n",
    "biases = {'out': tf.Variable(tf.random_normal([vocab_l]))}\n",
    "\n",
    "def RNN(x, weights, biases):\n",
    "    x = tf.reshape(x, [-1, n_input])\n",
    "    x = tf.split(x,n_input,1)\n",
    "    rnn_cell = rnn.BasicRNNCell(n_hidden, reuse =tf.AUTO_REUSE)\n",
    "    outputs, states = rnn.static_rnn(rnn_cell, x, dtype=tf.float32)\n",
    "    return tf.matmul(outputs[-1], weights['out']) + biases['out']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = RNN(x, weights, biases)\n",
    "\n",
    "\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=pred, labels=y))\n",
    "optimizer = tf.train.RMSPropOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "\n",
    "correct_pred = tf.equal(tf.argmax(pred,1), tf.argmax(y,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=pred, labels=y))\n",
    "optimizer = tf.train.RMSPropOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "# Model evaluation\n",
    "correct_pred = tf.equal(tf.argmax(pred,1), tf.argmax(y,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "\n",
    "# Initializing the variables\n",
    "init = tf.global_variables_initializer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Launch the graph\n",
    "with tf.Session() as session:\n",
    "    session.run(init)\n",
    "    step = 0\n",
    "    offset = random.randint(0,n_input+1)\n",
    "    end_offset = n_input + 1\n",
    "    acc_total = 0\n",
    "    loss_total = 0\n",
    "\n",
    "    writer.add_graph(session.graph)\n",
    "\n",
    "    while step < training_iters:\n",
    "        # Generate a minibatch. Add some randomness on selection process.\n",
    "        if offset > (len(train_data)-end_offset):\n",
    "            offset = random.randint(0, n_input+1)\n",
    "\n",
    "        symbols_in_keys = [ [dictionary[ str(train_data[i])]] for i in range(offset, offset+n_input) ]\n",
    "        symbols_in_keys = np.reshape(np.array(symbols_in_keys), [-1, n_input, 1])\n",
    "\n",
    "        symbols_out_onehot = np.zeros([vocab_size], dtype=float)\n",
    "        symbols_out_onehot[dictionary[str(train_data[offset+n_input])]] = 1.0\n",
    "        symbols_out_onehot = np.reshape(symbols_out_onehot,[1,-1])\n",
    "\n",
    "        _, acc, loss, onehot_pred = session.run([optimizer, accuracy, cost, pred], \\\n",
    "                                                feed_dict={x: symbols_in_keys, y: symbols_out_onehot})\n",
    "        loss_total += loss\n",
    "        acc_total += acc\n",
    "        if (step+1) % display_step == 0:\n",
    "            print(\"Iter= \" + str(step+1) + \", Average Loss= \" + \\\n",
    "                  \"{:.6f}\".format(loss_total/display_step) + \", Average Accuracy= \" + \\\n",
    "                  \"{:.2f}%\".format(100*acc_total/display_step))\n",
    "            acc_total = 0\n",
    "            loss_total = 0\n",
    "            symbols_in = [train_data[i] for i in range(offset, offset + n_input)]\n",
    "            symbols_out = train_data[offset + n_input]\n",
    "            symbols_out_pred = reverse_dictionary[int(tf.argmax(onehot_pred, 1).eval())]\n",
    "            print(\"%s - [%s] vs [%s]\" % (symbols_in,symbols_out,symbols_out_pred))\n",
    "        step += 1\n",
    "        offset += (n_input+1)\n",
    "    print(\"Optimization Finished!\")\n",
    "    print(\"Elapsed time: \", elapsed(time.time() - start_time))\n",
    "    \n",
    "    while True:\n",
    "        prompt = \"%s words: \" % n_input\n",
    "        sentence = input(prompt)\n",
    "        sentence = sentence.strip()\n",
    "        words = sentence.split(' ')\n",
    "        if len(words) != n_input:\n",
    "            continue\n",
    "        try:\n",
    "            symbols_in_keys = [dictionary[str(words[i])] for i in range(len(words))]\n",
    "            for i in range(32):\n",
    "                keys = np.reshape(np.array(symbols_in_keys), [-1, n_input, 1])\n",
    "                onehot_pred = session.run(pred, feed_dict={x: keys})\n",
    "                onehot_pred_index = int(tf.argmax(onehot_pred, 1).eval())\n",
    "                sentence = \"%s %s\" % (sentence,reverse_dictionary[onehot_pred_index])\n",
    "                symbols_in_keys = symbols_in_keys[1:]\n",
    "                symbols_in_keys.append(onehot_pred_index)\n",
    "            print(sentence)\n",
    "        except:\n",
    "            print(\"Word not in dictionary\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
